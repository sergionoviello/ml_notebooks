{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk.data\n",
    "from bs4 import BeautifulSoup  \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout\n",
    "from keras.models import Model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv( \"stories.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "train = train[train['summary'].notnull()]\n",
    "train['categories'].fillna('None', inplace=True)\n",
    "le = LabelEncoder()\n",
    "train['enc_category'] = le.fit_transform(train['categories'])\n",
    "len(train['categories'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "classes = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index, row in train.iterrows():\n",
    "    w = BeautifulSoup(row['summary'], \"lxml\")\n",
    "    texts.append(clean_str(w.get_text()))\n",
    "    labels.append(row['enc_category'])\n",
    "    classes.append(row['categories'])\n",
    "    \n",
    "labels = to_categorical(np.asarray(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergio/anaconda/lib/python3.5/site-packages/keras/preprocessing/text.py:90: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15075 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3880, 1000)\n",
      "(3880, 25)\n"
     ]
    }
   ],
   "source": [
    "labels = np.array(labels)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GLOVE_DIR = \"\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors in Glove 6B 100d.\n"
     ]
    }
   ],
   "source": [
    "print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergio/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:8: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(filters=128, kernel_size=3, activation=\"relu\")`\n",
      "/Users/sergio/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:8: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(filters=128, kernel_size=4, activation=\"relu\")`\n",
      "/Users/sergio/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:8: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(filters=128, kernel_size=5, activation=\"relu\")`\n",
      "/Users/sergio/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:12: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "convs = []\n",
    "filter_sizes = [3,4,5]\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "for fsz in filter_sizes:\n",
    "    l_conv = Conv1D(nb_filter=128,filter_length=fsz,activation='relu')(embedded_sequences)\n",
    "    l_pool = MaxPooling1D(5)(l_conv)\n",
    "    convs.append(l_pool)\n",
    "    \n",
    "l_merge = Merge(mode='concat', concat_axis=1)(convs)\n",
    "l_cov1= Conv1D(128, 5, activation='relu')(l_merge)\n",
    "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(30)(l_cov2)\n",
    "l_drop = Dropout((0.25))(l_pool2)\n",
    "l_flat = Flatten()(l_drop)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "preds = Dense(25, activation='softmax')(l_dense)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - simplified convolutional neural network\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 1000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 1000, 100)     1507600     input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 998, 128)      38528       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 997, 128)      51328       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)                (None, 996, 128)      64128       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)   (None, 199, 128)      0           conv1d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)   (None, 199, 128)      0           conv1d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)   (None, 199, 128)      0           conv1d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 597, 128)      0           max_pooling1d_1[0][0]            \n",
      "                                                                   max_pooling1d_2[0][0]            \n",
      "                                                                   max_pooling1d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)                (None, 593, 128)      82048       merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)   (None, 118, 128)      0           conv1d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)                (None, 114, 128)      82048       max_pooling1d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)   (None, 3, 128)        0           conv1d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 3, 128)        0           max_pooling1d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 384)           0           dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           49280       flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 25)            3225        dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 1,878,185\n",
      "Trainable params: 1,878,185\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergio/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:4: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3880 samples, validate on 969 samples\n",
      "Epoch 1/20\n",
      "3880/3880 [==============================] - 184s - loss: 2.5410 - acc: 0.2036 - val_loss: 2.0700 - val_acc: 0.3540\n",
      "Epoch 2/20\n",
      "3880/3880 [==============================] - 209s - loss: 1.9817 - acc: 0.3791 - val_loss: 1.8487 - val_acc: 0.4014\n",
      "Epoch 3/20\n",
      "3880/3880 [==============================] - 183s - loss: 1.7653 - acc: 0.4497 - val_loss: 1.6851 - val_acc: 0.4850\n",
      "Epoch 4/20\n",
      "3880/3880 [==============================] - 191s - loss: 1.5955 - acc: 0.4954 - val_loss: 1.6830 - val_acc: 0.4954\n",
      "Epoch 5/20\n",
      "3880/3880 [==============================] - 177s - loss: 1.4706 - acc: 0.5363 - val_loss: 1.5764 - val_acc: 0.5243\n",
      "Epoch 6/20\n",
      "3880/3880 [==============================] - 167s - loss: 1.3078 - acc: 0.5954 - val_loss: 1.6756 - val_acc: 0.5015\n",
      "Epoch 7/20\n",
      "3880/3880 [==============================] - 164s - loss: 1.1567 - acc: 0.6456 - val_loss: 1.5697 - val_acc: 0.5511\n",
      "Epoch 8/20\n",
      "3880/3880 [==============================] - 162s - loss: 1.0304 - acc: 0.6781 - val_loss: 1.6813 - val_acc: 0.5377\n",
      "Epoch 9/20\n",
      "3880/3880 [==============================] - 162s - loss: 0.8940 - acc: 0.7173 - val_loss: 1.5959 - val_acc: 0.5501\n",
      "Epoch 10/20\n",
      "3880/3880 [==============================] - 158s - loss: 0.7880 - acc: 0.7464 - val_loss: 1.8062 - val_acc: 0.5459\n",
      "Epoch 11/20\n",
      "3880/3880 [==============================] - 175s - loss: 0.6692 - acc: 0.7781 - val_loss: 1.8475 - val_acc: 0.5459\n",
      "Epoch 12/20\n",
      "3880/3880 [==============================] - 191s - loss: 0.5746 - acc: 0.8093 - val_loss: 2.0785 - val_acc: 0.5552\n",
      "Epoch 13/20\n",
      "3880/3880 [==============================] - 194s - loss: 0.4870 - acc: 0.8366 - val_loss: 1.8747 - val_acc: 0.5067\n",
      "Epoch 14/20\n",
      "3880/3880 [==============================] - 184s - loss: 0.4303 - acc: 0.8631 - val_loss: 2.3664 - val_acc: 0.5088\n",
      "Epoch 15/20\n",
      "3880/3880 [==============================] - 176s - loss: 0.3545 - acc: 0.8840 - val_loss: 2.4440 - val_acc: 0.5026\n",
      "Epoch 16/20\n",
      "3880/3880 [==============================] - 194s - loss: 0.3146 - acc: 0.8974 - val_loss: 2.6182 - val_acc: 0.5067\n",
      "Epoch 17/20\n",
      "3880/3880 [==============================] - 200s - loss: 0.2698 - acc: 0.9139 - val_loss: 3.1253 - val_acc: 0.4943\n",
      "Epoch 18/20\n",
      "3880/3880 [==============================] - 229s - loss: 0.2401 - acc: 0.9242 - val_loss: 3.0695 - val_acc: 0.5253\n",
      "Epoch 19/20\n",
      "3880/3880 [==============================] - 183s - loss: 0.1911 - acc: 0.9381 - val_loss: 3.3319 - val_acc: 0.4861\n",
      "Epoch 20/20\n",
      "3880/3880 [==============================] - 200s - loss: 0.1768 - acc: 0.9430 - val_loss: 4.1552 - val_acc: 0.4902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1422ed748>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"model fitting - simplified convolutional neural network\")\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=20, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_text = \"A breakdown of US Customs and Borders Protection computer systems caused chaos at airports around the United States on January 1.\"\n",
    "raw_text2 = \"One tough 12-year-old girl braved icy cold conditions to participate in her local polar plunge on January 1 in Hillsboro, Virginia.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "input = tokenizer.texts_to_sequences([raw_text])\n",
    "input2 = pad_sequences(input, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "pred = model.predict(input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weather cat; 99.977869% confidence\n"
     ]
    }
   ],
   "source": [
    "print(\"%s cat; %f%% confidence\" % (classes[np.argmax(pred)], pred[0][np.argmax(pred)] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unrest, conflicts and war cat; 99.998641% confidence\n"
     ]
    }
   ],
   "source": [
    "input3 = tokenizer.texts_to_sequences([raw_text2])\n",
    "input4 = pad_sequences(input3, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "pred2 = model.predict(input4)\n",
    "print(\"%s cat; %f%% confidence\" % (classes[np.argmax(pred2)], pred2[0][np.argmax(pred2)] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
