{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk.data\n",
    "from bs4 import BeautifulSoup  \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv( \"stories.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "train = train[train['summary'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "def pre_process_review(review, remove_stopwords=False):\n",
    "    text = BeautifulSoup(review, \"lxml\").get_text() # remove html\n",
    "    no_punc = re.sub(\"[^a-zA-Z]\", \" \", text) # remove punctuation\n",
    "    words = no_punc.lower().split() # lower case and split into words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))  \n",
    "        words = [w for w in words if not w in stops]\n",
    "    return words\n",
    "\n",
    "\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append( pre_process_review( raw_sentence, remove_stopwords ))\n",
    "    return sentences\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "\n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "map_uid = {}\n",
    "i = 0\n",
    "print(\"Parsing sentences from training set\")\n",
    "for idx, review in train.iterrows():\n",
    "\n",
    "    sentence = review_to_sentences(review[1] + ' ' + review[3], tokenizer)\n",
    "    key = \"SENT_{}\".format(review[0])\n",
    "    td = TaggedDocument(words=sentence[0], tags=[key])\n",
    "    sentences.append(td)\n",
    "    map_uid[key] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentences_perm(sentences):\n",
    "        shuffled = list(sentences)\n",
    "        random.shuffle(shuffled)\n",
    "        return(shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(sentences, alpha=0.025, min_alpha=0.025)  # use fixed learning rate\n",
    "model.train_words = False\n",
    "for epoch in range(10):\n",
    "    model.train(sentences_perm(sentences), total_examples=model.corpus_count)\n",
    "    model.alpha -= 0.002  # decrease the learning rate\n",
    "    model.min_alpha = model.alpha  # fix the learning rate, no decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save('stories_sim.doc2vec')\n",
    "# load the model back\n",
    "model_loaded = Doc2Vec.load('stories_sim.doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['this', 'couple', 'have', 'a', 'stranger', 'things', 'contract', 'to', 'ensure', 'no', 'spoilers', 'shall', 'ruin', 'season', 'the', 'time', 'has', 'almost', 'come'], tags=['SENT_185311'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_id = 3667 #np.random.randint(model_loaded.docvecs.count)\n",
    "sentences[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sims = model_loaded.docvecs.most_similar(doc_id, topn=model_loaded.docvecs.count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET (3667) (['SENT_185311']): «this couple have a stranger things contract to ensure no spoilers shall ruin season the time has almost come»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d100,n5,w5,mc5,s0.001,t3):\n",
      "\n",
      "0 ('SENT_187952', 0.5728790760040283): «lost teddy bear gets special treatment on flight back to its owner a young girl was reunited with her lost teddy bear thanks to a flight attendant who brought the toy from edinburgh to orkney on november»\n",
      "\n",
      "1 ('SENT_188098', 0.5676577687263489): «family of lynx visit anchorage alaska home alaska resident cathy newton captured footage of a family of lynx that visited her home in anchorage on october»\n",
      "\n",
      "2 ('SENT_185162', 0.5658762454986572): «cars drive through flooded streets in boone north carolina cleanup efforts continued in boone north carolina on october after flash flooding hit the area the previous day local media reported»\n",
      "\n",
      "3 ('SENT_185659', 0.5549824237823486): «polish plane aborts crosswind landing in austria returns to frankfurt a flight was forced to return to its point of origin frankfurt germany after a failed landing in salzburg austria during high winds brought by storm herwart on october»\n",
      "\n",
      "4 ('SENT_185727', 0.5531604290008545): «polish plane aborts crosswind landing in austria a polish enter air flight was forced to return to its point of origin frankfurt after a failed landing in salzburg austria during high winds brought by storm herwart on october»\n",
      "\n",
      "5 ('SENT_187557', 0.5504680275917053): «networks and message boards react to charles manson s death private networks have been rife with charles manson tributes following his death on november»\n",
      "\n",
      "6 ('SENT_190806', 0.548008382320404): «peter capaldi sends young doctor who fan reassuring letter about replacement a young doctor who fan in strabane northern ireland got a christmas surprise when he was sent a reassuring letter by former doctor who actor peter capaldi about capaldi s replacement on the show»\n",
      "\n",
      "7 ('SENT_189052', 0.5459877252578735): «creek fire burns near los angeles road a fire broke out on december scortching acres and forced thousands to evacuate their homes»\n",
      "\n",
      "8 ('SENT_184085', 0.5450328588485718): «cobh lashed by wind and waves as ophelia hits southern ireland cobh in county cork ireland was battered by strong winds and waves on october as ex hurricane ophelia began to hits parts of ireland»\n",
      "\n",
      "9 ('SENT_190023', 0.5422885417938232): «thousands of indonesians march in support of palestine muslim leaders called on indonesians to defend palestine during a large protest in jakarta on december over us president donald trump s decision to recognize jerusalem as the capital of israel local reports said»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(u'TARGET (%d) (%s): «%s»\\n' % (doc_id,sentences[doc_id].tags, ' '.join(sentences[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "# for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "#     idx = map_uid[sims[index][0]]\n",
    "#     print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(sentences[idx].words)))\n",
    "\n",
    "for index in range(100):\n",
    "    idx = map_uid[sims[index][0]]\n",
    "    print(u'%s %s: «%s»\\n' % (index, sims[index], ' '.join(sentences[idx].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
